{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParticleValue():\n",
    "    JSON_DATA = \"JSON_Data\"\n",
    "    ENG = \"eng\"\n",
    "    OK = \"ok\"\n",
    "    CHECKSUM_FAILED = \"checksum_failed\"\n",
    "    OUT_OF_RANGE = \"out_of_range\"\n",
    "    INVALID = \"invalid\"\n",
    "    QUESTIONABLE = \"questionable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParticle(object):\n",
    "    \"\"\"\n",
    "    This class is responsible for storing and ultimately generating data\n",
    "    particles in the designated format from the associated inputs. It\n",
    "    fills in fields as necessary, and is a valid Data Particle\n",
    "    that can be sent up to the InstrumentAgent.\n",
    "\n",
    "    It is the intent that this class is subclassed as needed if an instrument must\n",
    "    modify fields in the outgoing packet. The hope is to have most of the superclass\n",
    "    code be called by the child class with just values overridden as needed.\n",
    "    \"\"\"\n",
    "\n",
    "    # data particle type is intended to be defined in each derived data particle class.  This value should be unique\n",
    "    # for all data particles.  Best practice is to access this variable using the accessor method:\n",
    "    # data_particle_type()\n",
    "    _data_particle_type = None\n",
    "\n",
    "    def __init__(self, raw_data,\n",
    "                 port_timestamp=None,\n",
    "                 internal_timestamp=None,\n",
    "                 preferred_timestamp=DataParticleKey.PORT_TIMESTAMP,\n",
    "                 quality_flag=DataParticleValue.OK,\n",
    "                 new_sequence=None):\n",
    "        \"\"\" Build a particle seeded with appropriate information\n",
    "\n",
    "        @param raw_data The raw data used in the particle\n",
    "        \"\"\"\n",
    "        if new_sequence is not None and not isinstance(new_sequence, bool):\n",
    "            raise TypeError(\"new_sequence is not a bool\")\n",
    "\n",
    "        self.contents = {\n",
    "            DataParticleKey.PKT_FORMAT_ID: DataParticleValue.JSON_DATA,\n",
    "            DataParticleKey.PKT_VERSION: 1,\n",
    "            DataParticleKey.PORT_TIMESTAMP: port_timestamp,\n",
    "            DataParticleKey.INTERNAL_TIMESTAMP: internal_timestamp,\n",
    "            DataParticleKey.DRIVER_TIMESTAMP: ntplib.system_to_ntp_time(time.time()),\n",
    "            DataParticleKey.PREFERRED_TIMESTAMP: preferred_timestamp,\n",
    "            DataParticleKey.QUALITY_FLAG: quality_flag,\n",
    "        }\n",
    "        self._encoding_errors = []\n",
    "        if new_sequence is not None:\n",
    "            self.contents[DataParticleKey.NEW_SEQUENCE] = new_sequence\n",
    "\n",
    "        self.raw_data = raw_data\n",
    "\n",
    "    def __eq__(self, arg):\n",
    "        \"\"\"\n",
    "        Equality check for testing purposes.\n",
    "        \"\"\"\n",
    "        if self.data_particle_type() != arg.data_particle_type():\n",
    "            log.debug('Data particle type does not match: %s %s', self.data_particle_type(), arg.data_particle_type())\n",
    "            return False\n",
    "\n",
    "        if self.raw_data != arg.raw_data:\n",
    "            log.debug('Raw data does not match')\n",
    "            return False\n",
    "\n",
    "        generated1 = self.generate()\n",
    "        generated2 = arg.generate()\n",
    "        missing, differing = self._compare(generated1, generated2, ignore_keys=[DataParticleKey.DRIVER_TIMESTAMP,\n",
    "                                                                                DataParticleKey.PREFERRED_TIMESTAMP])\n",
    "        if missing:\n",
    "            log.error('Key mismatch between particle dictionaries: %r', missing)\n",
    "            return False\n",
    "\n",
    "        if differing:\n",
    "            log.error('Value mismatch between particle dictionaries: %r', differing)\n",
    "\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def _compare(d1, d2, ignore_keys=None):\n",
    "        ignore_keys = ignore_keys if ignore_keys else []\n",
    "        missing = set(d1).symmetric_difference(d2)\n",
    "        differing = {}\n",
    "        for k in d1:\n",
    "            if k in ignore_keys or k in missing:\n",
    "                continue\n",
    "            if d1[k] != d2[k]:\n",
    "                differing[k] = (d1[k], d2[k])\n",
    "\n",
    "        return missing, differing\n",
    "\n",
    "    @classmethod\n",
    "    def type(cls):\n",
    "        \"\"\"\n",
    "        return the data particle type\n",
    "        @return: data particle type\n",
    "        \"\"\"\n",
    "        return cls._data_particle_type\n",
    "\n",
    "    def set_internal_timestamp(self, timestamp=None, unix_time=None):\n",
    "        \"\"\"\n",
    "        Set the internal timestamp\n",
    "        @param timestamp: NTP timestamp to set\n",
    "        @param unix_time: Unix time as returned from time.time()\n",
    "        @raise InstrumentParameterException if timestamp or unix_time not supplied\n",
    "        \"\"\"\n",
    "        if timestamp is None and unix_time is None:\n",
    "            raise InstrumentParameterException(\"timestamp or unix_time required\")\n",
    "\n",
    "        if unix_time is not None:\n",
    "            timestamp = ntplib.system_to_ntp_time(unix_time)\n",
    "\n",
    "        self.contents[DataParticleKey.INTERNAL_TIMESTAMP] = float(timestamp)\n",
    "\n",
    "    def set_value(self, value_id, value):\n",
    "        \"\"\"\n",
    "        Set a content value, restricted as necessary\n",
    "\n",
    "        @param value_id The ID of the value to set, should be from DataParticleKey\n",
    "        @param value The value to set\n",
    "        @raises ReadOnlyException If the parameter cannot be set\n",
    "        \"\"\"\n",
    "        if (value_id == DataParticleKey.INTERNAL_TIMESTAMP) and (self._check_timestamp(value)):\n",
    "            self.contents[DataParticleKey.INTERNAL_TIMESTAMP] = value\n",
    "        else:\n",
    "            raise ReadOnlyException(\"Parameter %s not able to be set to %s after object creation!\" %\n",
    "                                    (value_id, value))\n",
    "\n",
    "    def get_value(self, value_id):\n",
    "        \"\"\" Return a stored value\n",
    "\n",
    "        @param value_id The ID (from DataParticleKey) for the parameter to return\n",
    "        @raises NotImplementedException If there is an invalid id\n",
    "        \"\"\"\n",
    "        if DataParticleKey.has(value_id):\n",
    "            return self.contents[value_id]\n",
    "        else:\n",
    "            raise NotImplementedException(\"Value %s not available in particle!\", value_id)\n",
    "\n",
    "    def data_particle_type(self):\n",
    "        \"\"\"\n",
    "        Return the data particle type (aka stream name)\n",
    "        @raise: NotImplementedException if _data_particle_type is not set\n",
    "        \"\"\"\n",
    "        if self._data_particle_type is None:\n",
    "            raise NotImplementedException(\"_data_particle_type not initialized\")\n",
    "\n",
    "        return self._data_particle_type\n",
    "\n",
    "    def generate_dict(self):\n",
    "        \"\"\"\n",
    "        Generate a simple dictionary of sensor data and timestamps, without\n",
    "        going to JSON. This is useful for the times when JSON is not needed to\n",
    "        go across an interface. There are times when particles are used\n",
    "        internally to a component/process/module/etc.\n",
    "        @retval A python dictionary with the proper timestamps and data values\n",
    "        @throws InstrumentDriverException if there is a problem wtih the inputs\n",
    "        \"\"\"\n",
    "        # Do we wan't downstream processes to check this?\n",
    "        # for time in [DataParticleKey.INTERNAL_TIMESTAMP,\n",
    "        #             DataParticleKey.DRIVER_TIMESTAMP,\n",
    "        #             DataParticleKey.PORT_TIMESTAMP]:\n",
    "        #    if  not self._check_timestamp(self.contents[time]):\n",
    "        #        raise SampleException(\"Invalid port agent timestamp in raw packet\")\n",
    "\n",
    "        # verify preferred timestamp exists in the structure...\n",
    "        if not self._check_preferred_timestamps():\n",
    "            raise SampleException(\"Preferred timestamp not in particle!\")\n",
    "\n",
    "        # build response structure\n",
    "        self._encoding_errors = []\n",
    "        values = self._build_parsed_values()\n",
    "\n",
    "        if all([self.contents[DataParticleKey.PREFERRED_TIMESTAMP] == DataParticleKey.PORT_TIMESTAMP,\n",
    "                self.contents[DataParticleKey.PORT_TIMESTAMP] == 0,\n",
    "                self.contents[DataParticleKey.INTERNAL_TIMESTAMP] is not None]):\n",
    "            self.contents[DataParticleKey.PREFERRED_TIMESTAMP] = DataParticleKey.INTERNAL_TIMESTAMP\n",
    "\n",
    "        result = self._build_base_structure()\n",
    "        result[DataParticleKey.STREAM_NAME] = self.data_particle_type()\n",
    "        result[DataParticleKey.VALUES] = values\n",
    "        return result\n",
    "\n",
    "    def generate(self, sorted=False):\n",
    "        \"\"\"\n",
    "        Generates a JSON_parsed packet from a sample dictionary of sensor data and\n",
    "        associates a timestamp with it\n",
    "\n",
    "        @param sorted ignored, maintained only to avoid breaking drivers\n",
    "        @return A dictionary representing this particle\n",
    "        @throws InstrumentDriverException If there is a problem with the inputs\n",
    "        \"\"\"\n",
    "        return self.generate_dict()\n",
    "\n",
    "    def _build_parsed_values(self):\n",
    "        \"\"\"\n",
    "        Build values of a parsed structure. Just the values are built so\n",
    "        so that a child class can override this class, but call it with\n",
    "        super() to get the base structure before modification\n",
    "\n",
    "        @return the values tag for this data structure ready to JSONify\n",
    "        @raises SampleException when parsed values can not be properly returned\n",
    "        \"\"\"\n",
    "        raise SampleException(\"Parsed values block not overridden\")\n",
    "\n",
    "    def _build_base_structure(self):\n",
    "        \"\"\"\n",
    "        Build the base/header information for an output structure.\n",
    "        Follow on methods can then modify it by adding or editing values.\n",
    "\n",
    "        @return A fresh copy of a core structure to be exported\n",
    "        \"\"\"\n",
    "        result = dict(self.contents)\n",
    "        # clean out optional fields that were missing\n",
    "        if not self.contents[DataParticleKey.PORT_TIMESTAMP]:\n",
    "            del result[DataParticleKey.PORT_TIMESTAMP]\n",
    "        if not self.contents[DataParticleKey.INTERNAL_TIMESTAMP]:\n",
    "            del result[DataParticleKey.INTERNAL_TIMESTAMP]\n",
    "        return result\n",
    "\n",
    "    def _check_timestamp(self, timestamp):\n",
    "        \"\"\"\n",
    "        Check to make sure the timestamp is reasonable\n",
    "\n",
    "        @param timestamp An NTP4 formatted timestamp (64bit)\n",
    "        @return True if timestamp is okay or None, False otherwise\n",
    "        \"\"\"\n",
    "        if timestamp is None:\n",
    "            return True\n",
    "        if not isinstance(timestamp, float):\n",
    "            return False\n",
    "\n",
    "        # is it sufficiently in the future to be unreasonable?\n",
    "        if timestamp > ntplib.system_to_ntp_time(time.time() + (86400 * 365)):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def _check_preferred_timestamps(self):\n",
    "        \"\"\"\n",
    "        Check to make sure the preferred timestamp indicated in the\n",
    "        particle is actually listed, possibly adjusting to 2nd best\n",
    "        if not there.\n",
    "\n",
    "        @throws SampleException When there is a problem with the preferred\n",
    "            timestamp in the sample.\n",
    "        \"\"\"\n",
    "        if self.contents[DataParticleKey.PREFERRED_TIMESTAMP] is None:\n",
    "            raise SampleException(\"Missing preferred timestamp, %s, in particle\" %\n",
    "                                  self.contents[DataParticleKey.PREFERRED_TIMESTAMP])\n",
    "\n",
    "        # This should be handled downstream.  Don't want to not publish data because\n",
    "        # the port agent stopped putting out timestamps\n",
    "        # if self.contents[self.contents[DataParticleKey.PREFERRED_TIMESTAMP]] == None:\n",
    "        #    raise SampleException(\"Preferred timestamp, %s, is not defined\" %\n",
    "        #                          self.contents[DataParticleKey.PREFERRED_TIMESTAMP])\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _encode_value(self, name, value, encoding_function):\n",
    "        \"\"\"\n",
    "        Encode a value using the encoding function, if it fails store the error in a queue\n",
    "        \"\"\"\n",
    "        encoded_val = None\n",
    "\n",
    "        try:\n",
    "            encoded_val = encoding_function(value)\n",
    "        except Exception:\n",
    "            log.error(\"Data particle error encoding. Name:%s Value:%s\", name, value)\n",
    "            self._encoding_errors.append({name: value})\n",
    "        return {DataParticleKey.VALUE_ID: name,\n",
    "                DataParticleKey.VALUE: encoded_val}\n",
    "\n",
    "    def get_encoding_errors(self):\n",
    "        \"\"\"\n",
    "        Return the encoding errors list\n",
    "        \"\"\"\n",
    "        return self._encoding_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-7db284bee7ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase64\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseEnum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSampleException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReadOnlyException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNotImplementedException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInstrumentParameterException\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\marian\\lib\\site-packages\\mi\\core\\common.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0m__license__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Apache 2.0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "# %load data_particle.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "@package mi.core.instrument.data_particle_generator Base data particle generator\n",
    "@file mi/core/instrument/data_particle_generator.py\n",
    "@author Steve Foley\n",
    "@brief Contains logic to generate data particles to be exchanged between\n",
    "the driver and agent. This involves a JSON interchange format\n",
    "\"\"\"\n",
    "import json\n",
    "import time\n",
    "import ntplib\n",
    "import base64\n",
    "\n",
    "from mi.core.common import BaseEnum\n",
    "from mi.core.exceptions import SampleException, ReadOnlyException, NotImplementedException, InstrumentParameterException\n",
    "from mi.core.log import get_logger\n",
    "\n",
    "\n",
    "__author__ = 'Steve Foley'\n",
    "__license__ = 'Apache 2.0'\n",
    "\n",
    "log = get_logger()\n",
    "\n",
    "\n",
    "class CommonDataParticleType(BaseEnum):\n",
    "    \"\"\"\n",
    "    This enum defines all the common particle types defined in the modules.  Currently there is only one, but by\n",
    "    using an enum here we have the opportunity to define more common data particles.\n",
    "    \"\"\"\n",
    "    RAW = \"raw\"\n",
    "\n",
    "\n",
    "class DataParticleKey(BaseEnum):\n",
    "    PKT_FORMAT_ID = \"pkt_format_id\"\n",
    "    PKT_VERSION = \"pkt_version\"\n",
    "    STREAM_NAME = \"stream_name\"\n",
    "    INTERNAL_TIMESTAMP = \"internal_timestamp\"\n",
    "    PORT_TIMESTAMP = \"port_timestamp\"\n",
    "    DRIVER_TIMESTAMP = \"driver_timestamp\"\n",
    "    PREFERRED_TIMESTAMP = \"preferred_timestamp\"\n",
    "    QUALITY_FLAG = \"quality_flag\"\n",
    "    VALUES = \"values\"\n",
    "    VALUE_ID = \"value_id\"\n",
    "    VALUE = \"value\"\n",
    "    BINARY = \"binary\"\n",
    "    NEW_SEQUENCE = \"new_sequence\"\n",
    "\n",
    "\n",
    "class DataParticleValue(BaseEnum):\n",
    "    JSON_DATA = \"JSON_Data\"\n",
    "    ENG = \"eng\"\n",
    "    OK = \"ok\"\n",
    "    CHECKSUM_FAILED = \"checksum_failed\"\n",
    "    OUT_OF_RANGE = \"out_of_range\"\n",
    "    INVALID = \"invalid\"\n",
    "    QUESTIONABLE = \"questionable\"\n",
    "\n",
    "\n",
    "class DataParticle(object):\n",
    "    \"\"\"\n",
    "    This class is responsible for storing and ultimately generating data\n",
    "    particles in the designated format from the associated inputs. It\n",
    "    fills in fields as necessary, and is a valid Data Particle\n",
    "    that can be sent up to the InstrumentAgent.\n",
    "\n",
    "    It is the intent that this class is subclassed as needed if an instrument must\n",
    "    modify fields in the outgoing packet. The hope is to have most of the superclass\n",
    "    code be called by the child class with just values overridden as needed.\n",
    "    \"\"\"\n",
    "\n",
    "    # data particle type is intended to be defined in each derived data particle class.  This value should be unique\n",
    "    # for all data particles.  Best practice is to access this variable using the accessor method:\n",
    "    # data_particle_type()\n",
    "    _data_particle_type = None\n",
    "\n",
    "    def __init__(self, raw_data,\n",
    "                 port_timestamp=None,\n",
    "                 internal_timestamp=None,\n",
    "                 preferred_timestamp=DataParticleKey.PORT_TIMESTAMP,\n",
    "                 quality_flag=DataParticleValue.OK,\n",
    "                 new_sequence=None):\n",
    "        \"\"\" Build a particle seeded with appropriate information\n",
    "\n",
    "        @param raw_data The raw data used in the particle\n",
    "        \"\"\"\n",
    "        if new_sequence is not None and not isinstance(new_sequence, bool):\n",
    "            raise TypeError(\"new_sequence is not a bool\")\n",
    "\n",
    "        self.contents = {\n",
    "            DataParticleKey.PKT_FORMAT_ID: DataParticleValue.JSON_DATA,\n",
    "            DataParticleKey.PKT_VERSION: 1,\n",
    "            DataParticleKey.PORT_TIMESTAMP: port_timestamp,\n",
    "            DataParticleKey.INTERNAL_TIMESTAMP: internal_timestamp,\n",
    "            DataParticleKey.DRIVER_TIMESTAMP: ntplib.system_to_ntp_time(time.time()),\n",
    "            DataParticleKey.PREFERRED_TIMESTAMP: preferred_timestamp,\n",
    "            DataParticleKey.QUALITY_FLAG: quality_flag,\n",
    "        }\n",
    "        self._encoding_errors = []\n",
    "        if new_sequence is not None:\n",
    "            self.contents[DataParticleKey.NEW_SEQUENCE] = new_sequence\n",
    "\n",
    "        self.raw_data = raw_data\n",
    "\n",
    "    def __eq__(self, arg):\n",
    "        \"\"\"\n",
    "        Equality check for testing purposes.\n",
    "        \"\"\"\n",
    "        if self.data_particle_type() != arg.data_particle_type():\n",
    "            log.debug('Data particle type does not match: %s %s', self.data_particle_type(), arg.data_particle_type())\n",
    "            return False\n",
    "\n",
    "        if self.raw_data != arg.raw_data:\n",
    "            log.debug('Raw data does not match')\n",
    "            return False\n",
    "\n",
    "        generated1 = self.generate()\n",
    "        generated2 = arg.generate()\n",
    "        missing, differing = self._compare(generated1, generated2, ignore_keys=[DataParticleKey.DRIVER_TIMESTAMP,\n",
    "                                                                                DataParticleKey.PREFERRED_TIMESTAMP])\n",
    "        if missing:\n",
    "            log.error('Key mismatch between particle dictionaries: %r', missing)\n",
    "            return False\n",
    "\n",
    "        if differing:\n",
    "            log.error('Value mismatch between particle dictionaries: %r', differing)\n",
    "\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def _compare(d1, d2, ignore_keys=None):\n",
    "        ignore_keys = ignore_keys if ignore_keys else []\n",
    "        missing = set(d1).symmetric_difference(d2)\n",
    "        differing = {}\n",
    "        for k in d1:\n",
    "            if k in ignore_keys or k in missing:\n",
    "                continue\n",
    "            if d1[k] != d2[k]:\n",
    "                differing[k] = (d1[k], d2[k])\n",
    "\n",
    "        return missing, differing\n",
    "\n",
    "    @classmethod\n",
    "    def type(cls):\n",
    "        \"\"\"\n",
    "        return the data particle type\n",
    "        @return: data particle type\n",
    "        \"\"\"\n",
    "        return cls._data_particle_type\n",
    "\n",
    "    def set_internal_timestamp(self, timestamp=None, unix_time=None):\n",
    "        \"\"\"\n",
    "        Set the internal timestamp\n",
    "        @param timestamp: NTP timestamp to set\n",
    "        @param unix_time: Unix time as returned from time.time()\n",
    "        @raise InstrumentParameterException if timestamp or unix_time not supplied\n",
    "        \"\"\"\n",
    "        if timestamp is None and unix_time is None:\n",
    "            raise InstrumentParameterException(\"timestamp or unix_time required\")\n",
    "\n",
    "        if unix_time is not None:\n",
    "            timestamp = ntplib.system_to_ntp_time(unix_time)\n",
    "\n",
    "        self.contents[DataParticleKey.INTERNAL_TIMESTAMP] = float(timestamp)\n",
    "\n",
    "    def set_value(self, value_id, value):\n",
    "        \"\"\"\n",
    "        Set a content value, restricted as necessary\n",
    "\n",
    "        @param value_id The ID of the value to set, should be from DataParticleKey\n",
    "        @param value The value to set\n",
    "        @raises ReadOnlyException If the parameter cannot be set\n",
    "        \"\"\"\n",
    "        if (value_id == DataParticleKey.INTERNAL_TIMESTAMP) and (self._check_timestamp(value)):\n",
    "            self.contents[DataParticleKey.INTERNAL_TIMESTAMP] = value\n",
    "        else:\n",
    "            raise ReadOnlyException(\"Parameter %s not able to be set to %s after object creation!\" %\n",
    "                                    (value_id, value))\n",
    "\n",
    "    def get_value(self, value_id):\n",
    "        \"\"\" Return a stored value\n",
    "\n",
    "        @param value_id The ID (from DataParticleKey) for the parameter to return\n",
    "        @raises NotImplementedException If there is an invalid id\n",
    "        \"\"\"\n",
    "        if DataParticleKey.has(value_id):\n",
    "            return self.contents[value_id]\n",
    "        else:\n",
    "            raise NotImplementedException(\"Value %s not available in particle!\", value_id)\n",
    "\n",
    "    def data_particle_type(self):\n",
    "        \"\"\"\n",
    "        Return the data particle type (aka stream name)\n",
    "        @raise: NotImplementedException if _data_particle_type is not set\n",
    "        \"\"\"\n",
    "        if self._data_particle_type is None:\n",
    "            raise NotImplementedException(\"_data_particle_type not initialized\")\n",
    "\n",
    "        return self._data_particle_type\n",
    "\n",
    "    def generate_dict(self):\n",
    "        \"\"\"\n",
    "        Generate a simple dictionary of sensor data and timestamps, without\n",
    "        going to JSON. This is useful for the times when JSON is not needed to\n",
    "        go across an interface. There are times when particles are used\n",
    "        internally to a component/process/module/etc.\n",
    "        @retval A python dictionary with the proper timestamps and data values\n",
    "        @throws InstrumentDriverException if there is a problem wtih the inputs\n",
    "        \"\"\"\n",
    "        # Do we wan't downstream processes to check this?\n",
    "        # for time in [DataParticleKey.INTERNAL_TIMESTAMP,\n",
    "        #             DataParticleKey.DRIVER_TIMESTAMP,\n",
    "        #             DataParticleKey.PORT_TIMESTAMP]:\n",
    "        #    if  not self._check_timestamp(self.contents[time]):\n",
    "        #        raise SampleException(\"Invalid port agent timestamp in raw packet\")\n",
    "\n",
    "        # verify preferred timestamp exists in the structure...\n",
    "        if not self._check_preferred_timestamps():\n",
    "            raise SampleException(\"Preferred timestamp not in particle!\")\n",
    "\n",
    "        # build response structure\n",
    "        self._encoding_errors = []\n",
    "        values = self._build_parsed_values()\n",
    "\n",
    "        if all([self.contents[DataParticleKey.PREFERRED_TIMESTAMP] == DataParticleKey.PORT_TIMESTAMP,\n",
    "                self.contents[DataParticleKey.PORT_TIMESTAMP] == 0,\n",
    "                self.contents[DataParticleKey.INTERNAL_TIMESTAMP] is not None]):\n",
    "            self.contents[DataParticleKey.PREFERRED_TIMESTAMP] = DataParticleKey.INTERNAL_TIMESTAMP\n",
    "\n",
    "        result = self._build_base_structure()\n",
    "        result[DataParticleKey.STREAM_NAME] = self.data_particle_type()\n",
    "        result[DataParticleKey.VALUES] = values\n",
    "        return result\n",
    "\n",
    "    def generate(self, sorted=False):\n",
    "        \"\"\"\n",
    "        Generates a JSON_parsed packet from a sample dictionary of sensor data and\n",
    "        associates a timestamp with it\n",
    "\n",
    "        @param sorted ignored, maintained only to avoid breaking drivers\n",
    "        @return A dictionary representing this particle\n",
    "        @throws InstrumentDriverException If there is a problem with the inputs\n",
    "        \"\"\"\n",
    "        return self.generate_dict()\n",
    "\n",
    "    def _build_parsed_values(self):\n",
    "        \"\"\"\n",
    "        Build values of a parsed structure. Just the values are built so\n",
    "        so that a child class can override this class, but call it with\n",
    "        super() to get the base structure before modification\n",
    "\n",
    "        @return the values tag for this data structure ready to JSONify\n",
    "        @raises SampleException when parsed values can not be properly returned\n",
    "        \"\"\"\n",
    "        raise SampleException(\"Parsed values block not overridden\")\n",
    "\n",
    "    def _build_base_structure(self):\n",
    "        \"\"\"\n",
    "        Build the base/header information for an output structure.\n",
    "        Follow on methods can then modify it by adding or editing values.\n",
    "\n",
    "        @return A fresh copy of a core structure to be exported\n",
    "        \"\"\"\n",
    "        result = dict(self.contents)\n",
    "        # clean out optional fields that were missing\n",
    "        if not self.contents[DataParticleKey.PORT_TIMESTAMP]:\n",
    "            del result[DataParticleKey.PORT_TIMESTAMP]\n",
    "        if not self.contents[DataParticleKey.INTERNAL_TIMESTAMP]:\n",
    "            del result[DataParticleKey.INTERNAL_TIMESTAMP]\n",
    "        return result\n",
    "\n",
    "    def _check_timestamp(self, timestamp):\n",
    "        \"\"\"\n",
    "        Check to make sure the timestamp is reasonable\n",
    "\n",
    "        @param timestamp An NTP4 formatted timestamp (64bit)\n",
    "        @return True if timestamp is okay or None, False otherwise\n",
    "        \"\"\"\n",
    "        if timestamp is None:\n",
    "            return True\n",
    "        if not isinstance(timestamp, float):\n",
    "            return False\n",
    "\n",
    "        # is it sufficiently in the future to be unreasonable?\n",
    "        if timestamp > ntplib.system_to_ntp_time(time.time() + (86400 * 365)):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def _check_preferred_timestamps(self):\n",
    "        \"\"\"\n",
    "        Check to make sure the preferred timestamp indicated in the\n",
    "        particle is actually listed, possibly adjusting to 2nd best\n",
    "        if not there.\n",
    "\n",
    "        @throws SampleException When there is a problem with the preferred\n",
    "            timestamp in the sample.\n",
    "        \"\"\"\n",
    "        if self.contents[DataParticleKey.PREFERRED_TIMESTAMP] is None:\n",
    "            raise SampleException(\"Missing preferred timestamp, %s, in particle\" %\n",
    "                                  self.contents[DataParticleKey.PREFERRED_TIMESTAMP])\n",
    "\n",
    "        # This should be handled downstream.  Don't want to not publish data because\n",
    "        # the port agent stopped putting out timestamps\n",
    "        # if self.contents[self.contents[DataParticleKey.PREFERRED_TIMESTAMP]] == None:\n",
    "        #    raise SampleException(\"Preferred timestamp, %s, is not defined\" %\n",
    "        #                          self.contents[DataParticleKey.PREFERRED_TIMESTAMP])\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _encode_value(self, name, value, encoding_function):\n",
    "        \"\"\"\n",
    "        Encode a value using the encoding function, if it fails store the error in a queue\n",
    "        \"\"\"\n",
    "        encoded_val = None\n",
    "\n",
    "        try:\n",
    "            encoded_val = encoding_function(value)\n",
    "        except Exception:\n",
    "            log.error(\"Data particle error encoding. Name:%s Value:%s\", name, value)\n",
    "            self._encoding_errors.append({name: value})\n",
    "        return {DataParticleKey.VALUE_ID: name,\n",
    "                DataParticleKey.VALUE: encoded_val}\n",
    "\n",
    "    def get_encoding_errors(self):\n",
    "        \"\"\"\n",
    "        Return the encoding errors list\n",
    "        \"\"\"\n",
    "        return self._encoding_errors\n",
    "\n",
    "\n",
    "class RawDataParticleKey(BaseEnum):\n",
    "    PAYLOAD = \"raw\"\n",
    "    LENGTH = \"length\"\n",
    "    TYPE = \"type\"\n",
    "    CHECKSUM = \"checksum\"\n",
    "\n",
    "\n",
    "class RawDataParticle(DataParticle):\n",
    "    \"\"\"\n",
    "    This class a common data particle for generating data particles of raw\n",
    "    data.\n",
    "\n",
    "    It essentially is a translation of the port agent packet\n",
    "    \"\"\"\n",
    "    _data_particle_type = CommonDataParticleType.RAW\n",
    "\n",
    "    def _build_parsed_values(self):\n",
    "        \"\"\"\n",
    "        Build a particle out of a port agent packet.\n",
    "        @returns A list that is ready to be added to the \"values\" tag before\n",
    "           the structure is JSONified\n",
    "        \"\"\"\n",
    "\n",
    "        port_agent_packet = self.raw_data\n",
    "        if not isinstance(port_agent_packet, dict):\n",
    "            raise SampleException(\"raw data not a dictionary\")\n",
    "\n",
    "        for param in [\"raw\", \"length\", \"type\", \"checksum\"]:\n",
    "            if param not in port_agent_packet:\n",
    "                raise SampleException(\"raw data not a complete port agent packet. missing %s\" % param)\n",
    "\n",
    "        payload = None\n",
    "        length = None\n",
    "        ptype = None\n",
    "        checksum = None\n",
    "\n",
    "        # Attempt to convert values\n",
    "        try:\n",
    "            payload = base64.b64encode(port_agent_packet.get(\"raw\"))\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            length = int(port_agent_packet.get(\"length\"))\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            ptype = int(port_agent_packet.get(\"type\"))\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            checksum = int(port_agent_packet.get(\"checksum\"))\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "        result = [{\n",
    "            DataParticleKey.VALUE_ID: RawDataParticleKey.PAYLOAD,\n",
    "            DataParticleKey.VALUE: payload,\n",
    "            DataParticleKey.BINARY: True},\n",
    "            {\n",
    "                DataParticleKey.VALUE_ID: RawDataParticleKey.LENGTH,\n",
    "                DataParticleKey.VALUE: length},\n",
    "            {\n",
    "                DataParticleKey.VALUE_ID: RawDataParticleKey.TYPE,\n",
    "                DataParticleKey.VALUE: ptype},\n",
    "            {\n",
    "                DataParticleKey.VALUE_ID: RawDataParticleKey.CHECKSUM,\n",
    "                DataParticleKey.VALUE: checksum},\n",
    "        ]\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load zplsc_c_echogram.py\n",
    "\"\"\"\n",
    "@package mi.dataset.driver.zplsc_c\n",
    "@file mi/dataset/driver/zplsc_c/zplsc_c_echogram.py\n",
    "@author Craig Risien/Rene Gelinas\n",
    "@brief ZPLSC Echogram generation for the ooicore\n",
    "\n",
    "Release notes:\n",
    "\n",
    "This class supports the generation of ZPLSC-C echograms.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import mi.dataset.driver.zplsc_c.zplsc_functions as zf\n",
    "\n",
    "__author__ = 'Rene Gelinas'\n",
    "\n",
    "\n",
    "class ZplscCParameters(object):\n",
    "    # TODO: This class should be replaced by methods to get the CCs from the system.\n",
    "    # Configuration Parameters\n",
    "    Salinity = 32   # Salinity in psu\n",
    "    Pressure = 150  # in dbars (~ depth of instrument in meters).\n",
    "    Bins2Avg = 1    # number of range bins to average - 1 is no averaging\n",
    "\n",
    "\n",
    "class ZplscCCalibrationCoefficients(object):\n",
    "    # TODO: This class should be replaced by methods to get the CCs from the system.\n",
    "    ka = 464.3636\n",
    "    kb = 3000.0\n",
    "    kc = 1.893\n",
    "    A = 0.001466\n",
    "    B = 0.0002388\n",
    "    C = 0.000000100335\n",
    "\n",
    "    TVR = []\n",
    "    VTX = []\n",
    "    BP = []\n",
    "    EL = []\n",
    "    DS = []\n",
    "\n",
    "    # Freq 38kHz\n",
    "    TVR.append(1.691999969482e2)\n",
    "    VTX.append(1.533999938965e2)\n",
    "    BP.append(8.609999902546e-3)\n",
    "    EL.append(1.623000030518e2)\n",
    "    DS.append(2.280000038445e-2)\n",
    "\n",
    "    # Freq 125kHz\n",
    "    TVR.append(1.668999938965e2)\n",
    "    VTX.append(5.8e+01)\n",
    "    BP.append(1.530999969691e-2)\n",
    "    EL.append(1.376999969482e2)\n",
    "    DS.append(2.280000038445e-2)\n",
    "\n",
    "    # Freq 200kHz\n",
    "    TVR.append(1.688999938965e2)\n",
    "    VTX.append(9.619999694824e1)\n",
    "    BP.append(1.530999969691e-2)\n",
    "    EL.append(1.456000061035e2)\n",
    "    DS.append(2.250000089407e-2)\n",
    "\n",
    "    # Freq 455kHz\n",
    "    TVR.append(1.696000061035e2)\n",
    "    VTX.append(1.301000061035e2)\n",
    "    BP.append(8.609999902546e-3)\n",
    "    EL.append(1.491999969482e2)\n",
    "    DS.append(2.300000004470e-2)\n",
    "\n",
    "\n",
    "class ZPLSCCEchogram(object):\n",
    "    def __init__(self):\n",
    "        self.cc = ZplscCCalibrationCoefficients()\n",
    "        self.params = ZplscCParameters()\n",
    "\n",
    "    def compute_backscatter(self, profile_hdr, chan_data, sound_speed, depth_range, sea_absorb):\n",
    "        \"\"\"\n",
    "        Compute the backscatter volumes values for one zplsc_c profile data record.\n",
    "        This code was borrowed from ASL MatLab code that reads in zplsc-c raw data\n",
    "        and performs calculations in order to compute the backscatter volume in db.\n",
    "\n",
    "        :param profile_hdr: Raw profile header with metadata from the zplsc-c instrument.\n",
    "        :param chan_data: Raw frequency data from the zplsc-c instrument.\n",
    "        :param sound_speed: Speed of sound at based on speed of sound, pressure and salinity.\n",
    "        :param depth_range: Range of the depth of the measurements\n",
    "        :param sea_absorb: Seawater absorption coefficient for each frequency\n",
    "        :return: sv: Volume backscatter in db\n",
    "        \"\"\"\n",
    "\n",
    "        __N = []\n",
    "        if self.params.Bins2Avg > 1:\n",
    "            for chan in range(profile_hdr.num_channels):\n",
    "                el = self.cc.EL[chan] - 2.5/self.cc.DS[chan] + np.array(chan_data[chan])/(26214*self.cc.DS[chan])\n",
    "                power = 10**(el/10)\n",
    "\n",
    "                # Perform bin averaging\n",
    "                num_bins = len(chan_data[chan])/self.params.Bins2Avg\n",
    "                pwr_avg = []\n",
    "                for _bin in range(num_bins):\n",
    "                    pwr_avg.append(np.mean(power[_bin*self.params.Bins2Avg:(_bin+1)*self.params.Bins2Avg]))\n",
    "\n",
    "                el_avg = 10*np.log10(pwr_avg)\n",
    "                __N.append(np.round(26214*self.cc.DS[chan]*(el_avg - self.cc.EL[chan] + 2.5/self.cc.DS[chan])))\n",
    "\n",
    "        else:\n",
    "            for chan in range(profile_hdr.num_channels):\n",
    "                __N.append(np.array(chan_data[chan]))\n",
    "\n",
    "        sv = []\n",
    "        for chan in range(profile_hdr.num_channels):\n",
    "            # Calculate correction to Sv due to non square transmit pulse\n",
    "            sv_offset = zf.compute_sv_offset(profile_hdr.frequency[chan], profile_hdr.pulse_length[chan])\n",
    "            sv.append(self.cc.EL[chan]-2.5/self.cc.DS[chan] + __N[chan]/(26214*self.cc.DS[chan]) - self.cc.TVR[chan] -\n",
    "                      20*np.log10(self.cc.VTX[chan]) + 20*np.log10(depth_range[chan]) +\n",
    "                      2*sea_absorb[chan]*depth_range[chan] -\n",
    "                      10*np.log10(0.5*sound_speed*profile_hdr.pulse_length[chan]/1e6*self.cc.BP[chan]) +\n",
    "                      sv_offset)\n",
    "\n",
    "        return sv\n",
    "\n",
    "    def compute_echogram_metadata(self, profile_hdr):\n",
    "        \"\"\"\n",
    "        Compute the metadata parameters needed to compute the zplsc-c volume backscatter values.\n",
    "\n",
    "        :param  profile_hdr: Raw profile header with metadata from the zplsc-c instrument.\n",
    "        :return: sound_speed : Speed of sound based on temperature, pressure and salinity.\n",
    "                 depth_range : Range of depth values of the zplsc-c data.\n",
    "                 sea_absorb : Sea absorption based on temperature, pressure, salinity and frequency.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the temperature sensor is available, compute the temperature from the counts.\n",
    "        temperature = 0\n",
    "        if profile_hdr.is_sensor_available:\n",
    "            temperature = zf.zplsc_c_temperature(profile_hdr.temperature, self.cc.ka, self.cc.kb, self.cc.kc,\n",
    "                                                 self.cc.A, self.cc.B, self.cc.C)\n",
    "\n",
    "        sound_speed = zf.zplsc_c_ss(temperature, self.params.Pressure, self.params.Salinity)\n",
    "\n",
    "        _m = []\n",
    "        depth_range = []\n",
    "        for chan in range(profile_hdr.num_channels):\n",
    "            _m.append(np.array([x for x in range(1, (profile_hdr.num_bins[chan]/self.params.Bins2Avg)+1)]))\n",
    "            depth_range.append(sound_speed*profile_hdr.lockout_index[0]/(2*profile_hdr.digitization_rate[0]) +\n",
    "                               (sound_speed/4)*(((2*_m[chan]-1)*profile_hdr.range_samples[0]*self.params.Bins2Avg-1) /\n",
    "                                                float(profile_hdr.digitization_rate[0]) +\n",
    "                                                profile_hdr.pulse_length[0]/1e6))\n",
    "\n",
    "        sea_absorb = []\n",
    "        for chan in range(profile_hdr.num_channels):\n",
    "            # Calculate absorption coefficient for each frequency.\n",
    "            sea_absorb.append(zf.zplsc_c_absorbtion(temperature, self.params.Pressure, self.params.Salinity,\n",
    "                                                    profile_hdr.frequency[chan]))\n",
    "\n",
    "        return sound_speed, depth_range, sea_absorb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from struct import unpack_from, unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParticleKey():\n",
    "    PKT_FORMAT_ID = \"pkt_format_id\"\n",
    "    PKT_VERSION = \"pkt_version\"\n",
    "    STREAM_NAME = \"stream_name\"\n",
    "    INTERNAL_TIMESTAMP = \"internal_timestamp\"\n",
    "    PORT_TIMESTAMP = \"port_timestamp\"\n",
    "    DRIVER_TIMESTAMP = \"driver_timestamp\"\n",
    "    PREFERRED_TIMESTAMP = \"preferred_timestamp\"\n",
    "    QUALITY_FLAG = \"quality_flag\"\n",
    "    VALUES = \"values\"\n",
    "    VALUE_ID = \"value_id\"\n",
    "    VALUE = \"value\"\n",
    "    BINARY = \"binary\"\n",
    "    NEW_SEQUENCE = \"new_sequence\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "    \"\"\" abstract class to show API needed for plugin poller objects \"\"\"\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_records(self, max_count):\n",
    "        \"\"\"\n",
    "        Returns a list of particles (following the instrument driver structure).\n",
    "        \"\"\"\n",
    "        raise NotImplementedException(\"get_records() not overridden!\")\n",
    "\n",
    "    def _publish_sample(self, samples):\n",
    "        \"\"\"\n",
    "        Publish the samples with the given publishing callback.\n",
    "        @param samples The list of data particle to publish up to the system\n",
    "        \"\"\"\n",
    "        if isinstance(samples, list):\n",
    "            self._publish_callback(samples)\n",
    "        else:\n",
    "            self._publish_callback([samples])\n",
    "\n",
    "    def _extract_sample(self, particle_class, regex, raw_data, port_timestamp=None, internal_timestamp=None,\n",
    "                        preferred_ts=DataParticleKey.INTERNAL_TIMESTAMP):\n",
    "        \"\"\"\n",
    "        Extract sample from a response line if present and publish\n",
    "        parsed particle\n",
    "\n",
    "        @param particle_class The class to instantiate for this specific\n",
    "            data particle. Parameterizing this allows for simple, standard\n",
    "            behavior from this routine\n",
    "        @param regex The regular expression that matches a data sample if regex\n",
    "                     is none then process every line\n",
    "        @param raw_data data to input into this particle.\n",
    "        @param port_timestamp the port_timestamp (default: None)\n",
    "        @param internal_timestamp the internal_timestamp (default: None)\n",
    "        @param preferred_ts the preferred timestamp (default: INTERNAL_TIMESTAMP)\n",
    "        @retval return a raw particle if a sample was found, else None\n",
    "        \"\"\"\n",
    "\n",
    "        particle = None\n",
    "\n",
    "        try:\n",
    "            if regex is None or regex.match(raw_data):\n",
    "                particle = particle_class(raw_data, port_timestamp=port_timestamp, internal_timestamp=internal_timestamp,\n",
    "                                          preferred_timestamp=preferred_ts)\n",
    "\n",
    "                # need to actually parse the particle fields to find out of there are errors\n",
    "                particle.generate_dict()\n",
    "                encoding_errors = particle.get_encoding_errors()\n",
    "                if encoding_errors:\n",
    "                    log.warn(\"Failed to encode: %s\", encoding_errors)\n",
    "                    raise SampleEncodingException(\"Failed to encode: %s\" % encoding_errors)\n",
    "\n",
    "        except (RecoverableSampleException, SampleEncodingException) as e:\n",
    "            log.error(\"Sample exception detected: %s raw data: %s\", e, raw_data)\n",
    "            if self._exception_callback:\n",
    "                self._exception_callback(e)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "        return particle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleParser(Parser):\n",
    "\n",
    "    def __init__(self, config, stream_handle, exception_callback):\n",
    "        \"\"\"\n",
    "        Initialize the simple parser, which does not use state or the chunker\n",
    "        and sieve functions.\n",
    "        @param config: The parser configuration dictionary\n",
    "        @param stream_handle: The stream handle of the file to parse\n",
    "        @param exception_callback: The callback to use when an exception occurs\n",
    "        \"\"\"\n",
    "\n",
    "        # the record buffer which will store all parsed particles\n",
    "        self._record_buffer = []\n",
    "        # a flag indicating if the file has been parsed or not\n",
    "        self._file_parsed = False\n",
    "\n",
    "\n",
    "\n",
    "    def parse_file(self):\n",
    "        \"\"\"\n",
    "        This method must be overridden.  This method should open and read the file and parser the data within, and at\n",
    "        the end of this method self._record_buffer will be filled with all the particles in the file.\n",
    "        \"\"\"\n",
    "        raise NotImplementedException(\"parse_file() not overridden!\")\n",
    "\n",
    "    def get_records(self, number_requested=1):\n",
    "        \"\"\"\n",
    "        Initiate parsing the file if it has not been done already, and pop particles off the record buffer to\n",
    "        return as many as requested if they are available in the buffer.\n",
    "        @param number_requested the number of records requested to be returned\n",
    "        @return an array of particles, with a length of the number requested or less\n",
    "        \"\"\"\n",
    "        particles_to_return = []\n",
    "\n",
    "        if number_requested > 0:\n",
    "            if self._file_parsed is False:\n",
    "                self.parse_file()\n",
    "                self._file_parsed = True\n",
    "\n",
    "        while len(particles_to_return) < number_requested and len(self._record_buffer) > 0:\n",
    "            particles_to_return.append(self._record_buffer.pop(0))\n",
    "\n",
    "        return particles_to_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROFILE_DATA_DELIMITER = '\\xfd\\x02'  # Byte Offset 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZplscCParticleKey():\n",
    "    \"\"\"\n",
    "    Class that defines fields that need to be extracted for the data particle.\n",
    "    \"\"\"\n",
    "    TRANS_TIMESTAMP = \"zplsc_c_transmission_timestamp\"\n",
    "    SERIAL_NUMBER = \"serial_number\"\n",
    "    PHASE = \"zplsc_c_phase\"\n",
    "    BURST_NUMBER = \"burst_number\"\n",
    "    TILT_X = \"zplsc_c_tilt_x_counts\"\n",
    "    TILT_Y = \"zplsc_c_tilt_y_counts\"\n",
    "    BATTERY_VOLTAGE = \"zplsc_c_battery_voltage_counts\"\n",
    "    TEMPERATURE = \"zplsc_c_temperature_counts\"\n",
    "    PRESSURE = \"zplsc_c_pressure_counts\"\n",
    "    IS_AVERAGED_DATA = \"zplsc_c_is_averaged_data\"\n",
    "    FREQ_CHAN_1 = \"zplsc_frequency_channel_1\"\n",
    "    VALS_CHAN_1 = \"zplsc_values_channel_1\"\n",
    "    DEPTH_CHAN_1 = \"zplsc_depth_range_channel_1\"\n",
    "    FREQ_CHAN_2 = \"zplsc_frequency_channel_2\"\n",
    "    VALS_CHAN_2 = \"zplsc_values_channel_2\"\n",
    "    DEPTH_CHAN_2 = \"zplsc_depth_range_channel_2\"\n",
    "    FREQ_CHAN_3 = \"zplsc_frequency_channel_3\"\n",
    "    VALS_CHAN_3 = \"zplsc_values_channel_3\"\n",
    "    DEPTH_CHAN_3 = \"zplsc_depth_range_channel_3\"\n",
    "    FREQ_CHAN_4 = \"zplsc_frequency_channel_4\"\n",
    "    VALS_CHAN_4 = \"zplsc_values_channel_4\"\n",
    "    DEPTH_CHAN_4 = \"zplsc_depth_range_channel_4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AzfpProfileHeader():\n",
    "    _pack_ = 1                              # 124 bytes in the header (includes the 2 byte delimiter)\n",
    "    _fields_ = [                            # V Byte Offset (from delimiter)\n",
    "        ('burst_num', 'i2'),            # 002 - Burst number\n",
    "        ('serial_num', 'i2'),           # 004 - Instrument Serial number\n",
    "        ('ping_status', 'i2'),          # 006 - Ping Status\n",
    "        ('burst_interval', 'i2'),         # 008 - Burst Interval (seconds)\n",
    "        ('year', 'i2'),                 # 012 - Year\n",
    "        ('month', 'i2'),                # 014 - Month\n",
    "        ('day', 'i2'),                  # 016 - Day\n",
    "        ('hour', 'i2'),                 # 018 - Hour\n",
    "        ('minute', 'i2'),               # 020 - Minute\n",
    "        ('second', 'i2'),               # 022 - Second\n",
    "        ('hundredths', 'i2'),           # 024 - Hundreths of a second\n",
    "        ('digitization_rate', 'i2'*4),  # 026 - Digitization Rate (channels 1-4) (64000, 40000 or 20000)\n",
    "        ('lockout_index', 'i2'*4),      # 034 - The sample number of samples skipped at start of ping (channels 1-4)\n",
    "        ('num_bins', 'i2'*4),           # 042 - Number of bins (channels 1-4)\n",
    "        ('range_samples', 'i2'*4),      # 050 - Range samples per bin (channels 1-4)\n",
    "        ('num_pings_profile', 'i2'),    # 058 - Number of pings per profile\n",
    "        ('is_averaged_pings', 'i2'),    # 060 - Indicates if pings are averaged in time\n",
    "        ('num_pings_burst', 'i2'),      # 062 - Number of pings that have been acquired in this burst\n",
    "        ('ping_period', 'i2'),          # 064 - Ping period in seconds\n",
    "        ('first_ping', 'i2'),           # 066 - First ping number (if averaged, first averaged ping number)\n",
    "        ('second_ping', 'i2'),          # 068 - Last ping number (if averaged, last averaged ping number)\n",
    "        ('is_averaged_data', 'i2'*4),    # 070 - 1 = averaged data (5 bytes), 0 = not averaged (2 bytes)\n",
    "        ('error_num', 'i2'),            # 074 - Error number if an error occurred\n",
    "        ('phase', 'i2'),                 # 076 - Phase used to acquire this profile\n",
    "        ('is_overrun', 'i2'),            # 077 - 1 if an over run occurred\n",
    "        ('num_channels', 'i2'),          # 078 - Number of channels (1, 2, 3 or 4)\n",
    "        ('gain', 'i2'*4),                # 079 - Gain (channels 1-4) 0, 1, 2, 3 (Obsolete)\n",
    "        ('spare', 'i2'),                 # 083 - Spare\n",
    "        ('pulse_length', 'i2'*4),       # 084 - Pulse length (channels 1-4) (uS)\n",
    "        ('board_num', 'i2''i2'*4),          # 092 - Board number of the data (channels 1-4)\n",
    "        ('frequency', 'i2'*4),          # 100 - Board frequency (channels 1-4)\n",
    "        ('is_sensor_available', 'i2'),  # 108 - Indicate if pressure/temperature sensor is available\n",
    "        ('tilt_x', 'i2'),               # 110 - Tilt X (counts)\n",
    "        ('tilt_y', 'i2'),               # 112 - Tilt Y (counts)\n",
    "        ('battery_voltage', 'i2'),      # 114 - Battery voltage (counts)\n",
    "        ('pressure', 'i2'),             # 116 - Pressure (counts)\n",
    "        ('temperature', 'i2'),          # 118 - Temperature (counts)\n",
    "        ('ad_channel_6', 'i2'),         # 120 - AD channel 6\n",
    "        ('ad_channel_7', 'i2')          # 122 - AD channel 7\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_file_path(filepath, output_path=None):\n",
    "    # Extract the file time from the file name\n",
    "    absolute_path = os.path.abspath(filepath)\n",
    "    filename = os.path.basename(absolute_path).upper()\n",
    "    directory_name = os.path.dirname(absolute_path)\n",
    "\n",
    "    output_path = directory_name if output_path is None else output_path\n",
    "    image_file = filename.replace('.01A', '.png')\n",
    "    return os.path.join(output_path, image_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZplscCCalibrationCoefficients(object):\n",
    "    # TODO: This class should be replaced by methods to get the CCs from the system.\n",
    "    DS = list()\n",
    "\n",
    "    # Freq 38kHz\n",
    "    DS.append(2.280000038445e-2)\n",
    "\n",
    "    # Freq 125kHz\n",
    "    DS.append(2.280000038445e-2)\n",
    "\n",
    "    # Freq 200kHz\n",
    "    DS.append(2.250000089407e-2)\n",
    "\n",
    "    # Freq 455kHz\n",
    "    DS.append(2.300000004470e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'METACLASS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-0014ded2bb30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mZplscCRecoveredDataParticle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataParticle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0m__metaclass__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMETACLASS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZplscCRecoveredDataParticle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-0014ded2bb30>\u001b[0m in \u001b[0;36mZplscCRecoveredDataParticle\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mZplscCRecoveredDataParticle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataParticle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0m__metaclass__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMETACLASS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZplscCRecoveredDataParticle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'METACLASS' is not defined"
     ]
    }
   ],
   "source": [
    "class ZplscCRecoveredDataParticle(DataParticle):\n",
    "    __metaclass__ = METACLASS\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ZplscCRecoveredDataParticle, self).__init__(*args, **kwargs)\n",
    "        self._data_particle_type = DataParticleType.ZPLSC_C_PARTICLE_TYPE\n",
    "\n",
    "    def _build_parsed_values(self):\n",
    "        \"\"\"\n",
    "        Build parsed values for Instrument Data Particle.\n",
    "        @return: list containing type encoded \"particle value id:value\" dictionary pairs\n",
    "        \"\"\"\n",
    "        # Particle Mapping table, where each entry is a tuple containing the particle\n",
    "        # field name, count(or count reference) and a function to use for data conversion.\n",
    "\n",
    "        port_timestamp = self.raw_data[ZplscCParticleKey.TRANS_TIMESTAMP]\n",
    "        self.contents[DataParticleKey.PORT_TIMESTAMP] = port_timestamp\n",
    "\n",
    "        return [{DataParticleKey.VALUE_ID: name, DataParticleKey.VALUE: None}\n",
    "                if self.raw_data[name] is None else\n",
    "                {DataParticleKey.VALUE_ID: name, DataParticleKey.VALUE: value}\n",
    "                for name, value in self.raw_data.iteritems()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZplscCParser(SimpleParser):\n",
    "    \n",
    "    def __init__(self, config, stream_handle, exception_callback):\n",
    "        super(ZplscCParser, self).__init__(config, stream_handle, exception_callback)\n",
    "        self._particle_type = None\n",
    "        self._gen = None\n",
    "        self.ph = None  # The profile header of the current record being processed.\n",
    "        self.cc = ZplscCCalibrationCoefficients()\n",
    "        self.is_first_record = True\n",
    "        self.hourly_avg_temp = 0\n",
    "        self.zplsc_echogram = ZPLSCCEchogram()\n",
    "        print('Hola from init')\n",
    "        print(stream_handle)\n",
    "        self._stream_handle=stream_handle\n",
    "        print(self._stream_handle)\n",
    "        \n",
    "    def find_next_record(self):\n",
    "        print('Hola from find_next_record')\n",
    "        good_delimiter = True\n",
    "        delimiter = self._stream_handle.read(2)\n",
    "        while delimiter not in [PROFILE_DATA_DELIMITER, '']:\n",
    "            \n",
    "            good_delimiter = False\n",
    "            delimiter = delimiter[1:2]\n",
    "            delimiter += self._stream_handle.read(1)\n",
    "        print('Adios while de find_next_record')\n",
    "        if not good_delimiter:\n",
    "            self._exception_callback('Invalid record delimiter found.\\n')\n",
    "        print('Adios find_next_record')\n",
    "    def parse_record(self):\n",
    "        \"\"\"\n",
    "        Parse one profile data record of the zplsc-c data file.\n",
    "        \"\"\"\n",
    "        chan_values = [[], [], [], []]\n",
    "        overflow_values = [[], [], [], []]\n",
    "\n",
    "        # Parse the data values portion of the record.\n",
    "        for chan in range(self.ph.num_channels):\n",
    "            num_bins = self.ph.num_bins[chan]\n",
    "\n",
    "            # Set the data structure format for the scientific data, based on whether\n",
    "            # the data is averaged or not. Construct the data structure and read the\n",
    "            # data bytes for the current channel. Unpack the data based on the structure.\n",
    "            if self.ph.is_averaged_data[chan]:\n",
    "                data_struct_format = '>' + str(num_bins) + 'I'\n",
    "            else:\n",
    "                data_struct_format = '>' + str(num_bins) + 'H'\n",
    "            data_struct = struct.Struct(data_struct_format)\n",
    "            data = self._stream_handle.read(data_struct.size)\n",
    "            chan_values[chan] = data_struct.unpack(data)\n",
    "\n",
    "            # If the data type is for averaged data, calculate the averaged data taking the\n",
    "            # the linear sum channel values and overflow values and using calculations from\n",
    "            # ASL MatLab code.\n",
    "            if self.ph.is_averaged_data[chan]:\n",
    "                overflow_struct_format = '>' + str(num_bins) + 'B'\n",
    "                overflow_struct = struct.Struct(overflow_struct_format)\n",
    "                overflow_data = self._stream_handle.read(num_bins)\n",
    "                overflow_values[chan] = overflow_struct.unpack(overflow_data)\n",
    "\n",
    "                if self.ph.is_averaged_pings:\n",
    "                    divisor = self.ph.num_pings_profile * self.ph.range_samples[chan]\n",
    "                else:\n",
    "                    divisor = self.ph.range_samples[chan]\n",
    "\n",
    "                linear_sum_values = np.array(chan_values[chan])\n",
    "                linear_overflow_values = np.array(overflow_values[chan])\n",
    "\n",
    "                values = (linear_sum_values + (linear_overflow_values * 0xFFFFFFFF))/divisor\n",
    "                values = (np.log10(values) - 2.5) * (8*0xFFFF) * self.cc.DS[chan]\n",
    "                values[np.isinf(values)] = 0\n",
    "                chan_values[chan] = values\n",
    "\n",
    "        # Convert the date and time parameters to a epoch time from 01-01-1900.\n",
    "        timestamp = (datetime(self.ph.year, self.ph.month, self.ph.day,\n",
    "                              self.ph.hour, self.ph.minute, self.ph.second,\n",
    "                              (self.ph.hundredths * 10000)) - datetime(1900, 1, 1)).total_seconds()\n",
    "\n",
    "        sound_speed, depth_range, sea_absorb = self.zplsc_echogram.compute_echogram_metadata(self.ph)\n",
    "\n",
    "        chan_values = self.zplsc_echogram.compute_backscatter(self.ph, chan_values, sound_speed, depth_range,\n",
    "                                                              sea_absorb)\n",
    "\n",
    "        zplsc_particle_data = {\n",
    "            ZplscCParticleKey.TRANS_TIMESTAMP: timestamp,\n",
    "            ZplscCParticleKey.SERIAL_NUMBER: str(self.ph.serial_num),\n",
    "            ZplscCParticleKey.PHASE: self.ph.phase,\n",
    "            ZplscCParticleKey.BURST_NUMBER: self.ph.burst_num,\n",
    "            ZplscCParticleKey.TILT_X: self.ph.tilt_x,\n",
    "            ZplscCParticleKey.TILT_Y: self.ph.tilt_y,\n",
    "            ZplscCParticleKey.BATTERY_VOLTAGE: self.ph.battery_voltage,\n",
    "            ZplscCParticleKey.PRESSURE: self.ph.pressure,\n",
    "            ZplscCParticleKey.TEMPERATURE: self.ph.temperature,\n",
    "            ZplscCParticleKey.IS_AVERAGED_DATA: list(self.ph.is_averaged_data),\n",
    "            ZplscCParticleKey.FREQ_CHAN_1: float(self.ph.frequency[0]),\n",
    "            ZplscCParticleKey.VALS_CHAN_1: list(chan_values[0]),\n",
    "            ZplscCParticleKey.DEPTH_CHAN_1: list(depth_range[0]),\n",
    "            ZplscCParticleKey.FREQ_CHAN_2: float(self.ph.frequency[1]),\n",
    "            ZplscCParticleKey.VALS_CHAN_2: list(chan_values[1]),\n",
    "            ZplscCParticleKey.DEPTH_CHAN_2: list(depth_range[1]),\n",
    "            ZplscCParticleKey.FREQ_CHAN_3: float(self.ph.frequency[2]),\n",
    "            ZplscCParticleKey.VALS_CHAN_3: list(chan_values[2]),\n",
    "            ZplscCParticleKey.DEPTH_CHAN_3: list(depth_range[2]),\n",
    "            ZplscCParticleKey.FREQ_CHAN_4: float(self.ph.frequency[3]),\n",
    "            ZplscCParticleKey.VALS_CHAN_4: list(chan_values[3]),\n",
    "            ZplscCParticleKey.DEPTH_CHAN_4: list(depth_range[3])\n",
    "        }\n",
    "\n",
    "        return zplsc_particle_data, timestamp, chan_values, depth_range\n",
    "\n",
    "    def parse_file(self):\n",
    "        self.ph = AzfpProfileHeader()\n",
    "        self.find_next_record()\n",
    "        while self._stream_handle.readinto(self.ph):\n",
    "            try:\n",
    "                # Parse the current record\n",
    "                zplsc_particle_data, timestamp, _, _ = self.parse_record()\n",
    "\n",
    "                # Create the data particle\n",
    "                particle = self._extract_sample(ZplscCRecoveredDataParticle, None, zplsc_particle_data, timestamp,\n",
    "                                                timestamp, DataParticleKey.PORT_TIMESTAMP)\n",
    "                if particle is not None:\n",
    "                    log.trace('Parsed particle: %s' % particle.generate_dict())\n",
    "                    self._record_buffer.append(particle)\n",
    "\n",
    "            except (IOError, OSError) as ex:\n",
    "                self._exception_callback('Reading stream handle: %s: %s\\n' % (self._stream_handle.name, ex.message))\n",
    "                return\n",
    "            except struct.error as ex:\n",
    "                self._exception_callback('Unpacking the data from the data structure: %s' % ex.message)\n",
    "            except exceptions.ValueError as ex:\n",
    "                self._exception_callback('Transition timestamp has invalid format: %s' % ex.message)\n",
    "            except (SampleException, RecoverableSampleException) as ex:\n",
    "                self._exception_callback('Creating data particle: %s' % ex.message)\n",
    "\n",
    "            # Clear the profile header data structure and find the next record.\n",
    "            self.ph = AzfpProfileHeader()\n",
    "            self.find_next_record()\n",
    "            \n",
    "\n",
    "    def create_echogram(self, echogram_file_path=None):\n",
    "        \"\"\"\n",
    "        Parse the *.O1A zplsc_c data file and create the echogram from this data.\n",
    "        :param echogram_file_path: Path to store the echogram locally.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        import logging\n",
    "        sv_dict = {}\n",
    "        data_times = []\n",
    "        frequencies = {}\n",
    "        depth_range = []\n",
    "        \n",
    "        print('Hola from create echogram')\n",
    "        \n",
    "        input_file_path ='18030100.01A'#self._stream_handle.name\n",
    "        logging.info('Begin processing echogram data: %r', input_file_path)\n",
    "        image_path = generate_image_file_path(input_file_path, echogram_file_path)\n",
    "\n",
    "        self.ph = AzfpProfileHeader()\n",
    "        self.find_next_record()\n",
    "        while self._stream_handle.readinto(self.ph):\n",
    "            try:\n",
    "                _, timestamp, chan_data, depth_range = self.parse_record()\n",
    "\n",
    "                if not sv_dict:\n",
    "                    range_chan_data = range(1, len(chan_data)+1)\n",
    "                    sv_dict = {channel: [] for channel in range_chan_data}\n",
    "                    frequencies = {channel: float(self.ph.frequency[channel-1]) for channel in range_chan_data}\n",
    "                    print('Hola desde while de create echogram1')\n",
    "                for channel in sv_dict:\n",
    "                    sv_dict[channel].append(chan_data[channel-1])\n",
    "                    print('Hola desde while de create echogram2')\n",
    "                data_times.append(timestamp)\n",
    "\n",
    "            except (IOError, OSError) as ex:\n",
    "                self._exception_callback(ex)\n",
    "                return\n",
    "            except struct.error as ex:\n",
    "                self._exception_callback(ex)\n",
    "            except exceptions.ValueError as ex:\n",
    "                self._exception_callback(ex)\n",
    "            except (SampleException, RecoverableSampleException) as ex:\n",
    "                self._exception_callback(ex)\n",
    "\n",
    "            # Clear the profile header data structure and find the next record.\n",
    "            self.ph = AzfpProfileHeader()\n",
    "            self.find_next_record()\n",
    "\n",
    "        logging.info('Completed processing all data: %r', input_file_path)\n",
    "\n",
    "        data_times = np.array(data_times)\n",
    "\n",
    "        for channel in sv_dict:\n",
    "            sv_dict[channel] = np.array(sv_dict[channel])\n",
    "\n",
    "        logging.info('Begin generating echogram: %r', image_path)\n",
    "\n",
    "        plot = ZPLSPlot(data_times, sv_dict, frequencies, depth_range[0][-1], depth_range[0][0])\n",
    "        plot.generate_plots()\n",
    "        plot.write_image(image_path)\n",
    "\n",
    "        log.info('Completed generating echogram: %r', image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleDataHandler(object):\n",
    "    def __init__(self):particle_data_handler\n",
    "    def addParticleSample(self, sample_type, sample):\n",
    "        log.debug(\"Sample type: %s, Sample data: %s\", sample_type, sample)\n",
    "        self._samples.setdefault(sample_type, []).append(sample)\n",
    "\n",
    "    def setParticleDataCaptureFailure(self):\n",
    "        log.debug(\"Particle data capture failed\")\n",
    "        self._failure = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_exception_callback(exception):\n",
    "        \"\"\"\n",
    "        Callback function to log exceptions and continue.\n",
    "\n",
    "        @param exception - Exception that occurred\n",
    "        \"\"\"\n",
    "\n",
    "        log.info(\"Exception occurred: %s\", exception.message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetDriverConfigKeys():\n",
    "    PARTICLE_MODULE = \"particle_module\"\n",
    "    PARTICLE_CLASS = \"particle_class\"\n",
    "    PARTICLE_CLASSES_DICT = \"particle_classes_dict\"\n",
    "    DIRECTORY = \"directory\"\n",
    "    STORAGE_DIRECTORY = \"storage_directory\"\n",
    "    PATTERN = \"pattern\"\n",
    "    FREQUENCY = \"frequency\"\n",
    "    FILE_MOD_WAIT_TIME = \"file_mod_wait_time\"\n",
    "    HARVESTER = \"harvester\"\n",
    "    PARSER = \"parser\"\n",
    "    MODULE = \"module\"\n",
    "    CLASS = \"class\"\n",
    "    URI = \"uri\"\n",
    "    CLASS_ARGS = \"class_args\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prueba\n"
     ]
    }
   ],
   "source": [
    "print('prueba')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola from init\n",
      "<_io.BufferedReader name='18030100.01A'>\n",
      "<_io.BufferedReader name='18030100.01A'>\n",
      "Hola from create echogram\n",
      "Hola from find_next_record\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-812dadcedecc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mZplscCParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrec_exception_callback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mzplsc_echogram_file_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\Oceanhackweek\\proyecto\\AFZP_matlab'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_echogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzplsc_echogram_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-92-86b4260ce657>\u001b[0m in \u001b[0;36mcreate_echogram\u001b[1;34m(self, echogram_file_path)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAzfpProfileHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_next_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream_handle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-92-86b4260ce657>\u001b[0m in \u001b[0;36mfind_next_record\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mgood_delimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mdelimiter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream_handle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Adios while de find_next_record'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgood_delimiter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_file_path = '18030100.01A'\n",
    "MODULE_NAME = 'mi.dataset.parser.zplsc_c'\n",
    "CLASS_NAME = 'ZplscCRecoveredDataParticle'\n",
    "CONFIG = {\n",
    "    DataSetDriverConfigKeys.PARTICLE_MODULE: MODULE_NAME,\n",
    "    DataSetDriverConfigKeys.PARTICLE_CLASS: CLASS_NAME\n",
    "}\n",
    "#Nor sure if the first ZplscCParser argument should be CONFIG or None.\n",
    "zplsc_echogram_file_path = 'C:\\Oceanhackweek\\proyecto\\AFZP_matlab'\n",
    "parser = ZplscCParser(None,  open(input_file_path, 'rb'), rec_exception_callback)\n",
    "zplsc_echogram_file_path = 'C:\\Oceanhackweek\\proyecto\\AFZP_matlab'\n",
    "parser.create_echogram(zplsc_echogram_file_path) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sv_offset(frequency, pulse_length):\n",
    "    \"\"\"\n",
    "    A correction must be made to compensate for the effects of the finite response\n",
    "    times of both the receiving and transmitting parts of the instrument. The magnitude\n",
    "    of the correction will depend on the length of the transmitted pulse, and the response\n",
    "    time (on both transmission and reception) of the instrument.\n",
    "\n",
    "    :param frequency: Frequency in KHz\n",
    "    :param pulse_length: Pulse length in uSecs\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    sv_offset = 0\n",
    "\n",
    "    if frequency > 38:  # 125,200,455,769 kHz\n",
    "        if pulse_length == 300:\n",
    "            sv_offset = 1.1\n",
    "        elif pulse_length == 500:\n",
    "            sv_offset = 0.8\n",
    "        elif pulse_length == 700:\n",
    "            sv_offset = 0.5\n",
    "        elif pulse_length == 900:\n",
    "            sv_offset = 0.3\n",
    "        elif pulse_length == 1000:\n",
    "            sv_offset = 0.3\n",
    "    else:  # 38 kHz\n",
    "        if pulse_length == 500:\n",
    "            sv_offset = 1.1\n",
    "        elif pulse_length == 1000:\n",
    "            sv_offset = 0.7\n",
    "\n",
    "    return sv_offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-71-aa1b93d6ffc6>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-71-aa1b93d6ffc6>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    sea_c = 1449.05 + (z * (45.7 + z*((-5.21) + 0.23*z))) + ((1.333 + z*((-0.126) + z*0.009)) * (s-35.0)) + \\(p/1000)*(16.3+0.18*(p/1000))\u001b[0m\n\u001b[1;37m                                                                                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# de fichero  C:\\Oceanhackweek\\proyecto\\AFZP_matlab\\mi_instrument\\mi\\dataset\\driver\\zplsc_c\\zplsc_functions.py\n",
    "decompress_power = np.array(power) * 10. * np.log10(2) / 256.\n",
    "vin = 2.5 * (counts / 65535)\n",
    "r = (ka + kb*vin) / (kc - vin)\n",
    "temperature = 1 / (a + b * (np.log(r)) + c * (np.log(r)**3)) - 273\n",
    "tilt = a + (b * counts) + (c * counts**2) + (d * counts**3)\n",
    "z = t/10\n",
    "sea_c = 1449.05 + (z * (45.7 + z*((-5.21) + 0.23*z))) + ((1.333 + z*((-0.126) + z*0.009)) * (s-35.0)) + \\(p/1000)*(16.3+0.18*(p/1000))\n",
    "# Calculate relaxation frequencies\n",
    "t_k = t + 273.0\n",
    "f1 = 1320.0*t_k * np.exp(-1700/t_k)\n",
    "f2 = 1.55e7*t_k * np.exp(-3052/t_k)\n",
    "\n",
    "# Coefficients for absorption equations\n",
    "k = 1 + p/10.0\n",
    "a = 8.95e-8 * (1 + t*(2.29e-2 - 5.08e-4*t))\n",
    "b = (s/35.0)*4.88e-7*(1+0.0134*t)*(1-0.00103*k + 3.7e-7*(k*k))\n",
    "c = 4.86e-13*(1+t*((-0.042)+t*(8.53e-4-t*6.23e-6)))*(1+k*(-3.84e-4+k*7.57e-8))\n",
    "freqk = freq*1000\n",
    "sea_abs = (a*f1*(freqk**2))/((f1*f1)+(freqk**2))+(b*f2*(freqk**2))/((f2*f2)+(freqk**2))+c*(freqk**2)\n",
    "\n",
    "#aplicar compute_sv_offset(frequency, pulse_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set contants for unpacking .raw files\n",
    "#BLOCK_SIZE = 1024*4             # Block size read in from binary file to search for token\n",
    "#LENGTH_SIZE = 4\n",
    " #   byte_cnt = LENGTH_SIZE\n",
    "#\n",
    "    # Configuration datagram header\n",
    " #   byte_cnt += DATAGRAM_HEADER_SIZE\n",
    "#\n",
    "    # Configuration: header\n",
    "  #  config_header = read_config_header(raw[byte_cnt:byte_cnt+CONFIG_HEADER_SIZE])\n",
    "   # byte_cnt += CONFIG_HEADER_SIZE\n",
    "   #config_transducer = []\n",
    "    #for num_transducer in range(config_header['transducer_count']):\n",
    "     #   config_transducer.append(read_config_transducer(raw[byte_cnt:byte_cnt+CONFIG_TRANSDUCER_SIZE]))\n",
    "      #  byte_cnt += CONFIG_TRANSDUCER_SIZE\n",
    "   \n",
    "\n",
    "    #raw = filehandle.read(BLOCK_SIZE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'power_data_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-266564cdaa0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpower_data_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m38000.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maspect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'power_data_dict' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(power_data_dict[38000.0],aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(power_data_dict[120000.0],aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
